#!/usr/bin/env python3
"""å¤§è§„æ¨¡å•æ— äººæœºç¯å¢ƒ - æ¯ä¸ªç¯å¢ƒä¸€æ¶æ— äººæœºï¼Œé€šè¿‡å¤§é‡å¹¶è¡Œç¯å¢ƒæé«˜GPUåˆ©ç”¨ç‡"""

import gymnasium as gym
import numpy as np
from gymnasium import spaces
from scene_creation import DroneScene
from reward_system import RewardSystem
import pybullet as p
from stable_baselines3.common.vec_env import SubprocVecEnv
from typing import List, Optional, Tuple, Union
import multiprocessing as mp

class SingleDronePathPlanningEnv(gym.Env):
    """å•æ— äººæœºè·¯å¾„è§„åˆ’RLç¯å¢ƒ - ä¸“ä¸ºå¤§è§„æ¨¡å¹¶è¡Œè®­ç»ƒè®¾è®¡"""
    
    def __init__(self, ray_length=1000.0, env_id=0):
        super().__init__()
        self.env_id = env_id  # ç¯å¢ƒIDï¼Œç”¨äºåŒºåˆ†ä¸åŒç¯å¢ƒ
        
        # åˆ›å»ºåœºæ™¯
        self.scene = DroneScene(gui=(self.env_id==0))  # åªè®©ç¬¬0å·ç¯å¢ƒå¼€GUI
        self.scene.create_obstacles()
        # ä¸ºå¤§è§„æ¨¡è®­ç»ƒåˆ›å»ºæ›´å¤šç›®æ ‡ç‚¹
        self.scene.create_targets(num_targets=128, area=8000)
        
        # åˆ›å»ºå•ä¸ªæ— äººæœº
        self.drone_id = self.scene.create_drone()
        
        # åŠ¨ä½œç©ºé—´ï¼š3ä¸ªåŠ¨ä½œ [dx, dy, dz]ï¼ŒèŒƒå›´[-1, 1]ç±³
        self.action_space = spaces.Box(
            low=-1, high=1, 
            shape=(3,), 
            dtype=np.float32
        )
        
        # çŠ¶æ€ç©ºé—´ï¼š35ç»´çŠ¶æ€
        # 3(ä½ç½®) + 3(ç›®æ ‡) + 26(å°„çº¿è·ç¦») + 3(é€Ÿåº¦)
        self.observation_space = spaces.Box(
            low=-10000, high=10000, 
            shape=(35,), 
            dtype=np.float32
        )
        
        self.max_steps = 50000  # æ¯å›åˆæœ€å¤§æ­¥æ•°
        self.current_step = 0
        self.max_energy = 15000.0
        self.energy = self.max_energy
        self.reward_system = RewardSystem()
        self.previous_position = None
        self.ray_length = ray_length
        
        # å®šä¹‰26ä¸ªæ–¹å‘çš„å°„çº¿
        self.ray_directions = self._generate_ray_directions()
        
        # ä¸ºæ¯ä¸ªç¯å¢ƒåˆ†é…ä¸åŒçš„ç›®æ ‡ç‚¹
        self.target = self._assign_target_by_env_id()
        
        # è®°å½•ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'episodes': 0,
            'total_reward': 0.0,  # æ”¹ä¸ºfloat
            'targets_reached': 0,
            'collisions': 0,
            'avg_episode_length': 0.0  # æ”¹ä¸ºfloat
        }
        self.episode_reward = 0  # æ–°å¢ï¼šç´¯è®¡æœ¬å›åˆå¥–åŠ±

    def _generate_ray_directions(self):
        """ç”Ÿæˆ26ä¸ªæ–¹å‘çš„å°„çº¿å‘é‡"""
        directions = []
        
        # 6ä¸ªä¸»è¦æ–¹å‘ï¼šÂ±X, Â±Y, Â±Z
        main_dirs = [
            [1, 0, 0], [-1, 0, 0],  # +X, -X
            [0, 1, 0], [0, -1, 0],  # +Y, -Y  
            [0, 0, 1], [0, 0, -1]   # +Z, -Z
        ]
        directions.extend(main_dirs)
        
        # 12ä¸ªè¾¹æ–¹å‘ï¼šæ¯ä¸ªåæ ‡è½´çš„æ­£è´Ÿç»„åˆ
        edge_dirs = [
            [1, 1, 0], [1, -1, 0], [-1, 1, 0], [-1, -1, 0],  # XYå¹³é¢
            [1, 0, 1], [1, 0, -1], [-1, 0, 1], [-1, 0, -1],  # XZå¹³é¢
            [0, 1, 1], [0, 1, -1], [0, -1, 1], [0, -1, -1]   # YZå¹³é¢
        ]
        directions.extend(edge_dirs)
        
        # 8ä¸ªè§’æ–¹å‘ï¼šä¸‰ä¸ªåæ ‡è½´çš„æ­£è´Ÿç»„åˆ
        corner_dirs = [
            [1, 1, 1], [1, 1, -1], [1, -1, 1], [1, -1, -1],
            [-1, 1, 1], [-1, 1, -1], [-1, -1, 1], [-1, -1, -1]
        ]
        directions.extend(corner_dirs)
        
        # å½’ä¸€åŒ–æ‰€æœ‰æ–¹å‘å‘é‡
        normalized_dirs = []
        for direction in directions:
            norm = np.linalg.norm(direction)
            normalized_dirs.append([d/norm for d in direction])
        
        return normalized_dirs

    def _assign_target_by_env_id(self):
        """æ ¹æ®ç¯å¢ƒIDåˆ†é…ç›®æ ‡ç‚¹ï¼Œç¡®ä¿ä¸åŒç¯å¢ƒæœ‰ä¸åŒçš„ç›®æ ‡"""
        if not self.scene.targets:
            return [0, 0, 2.5]  # é»˜è®¤ç›®æ ‡
        
        # ä½¿ç”¨ç¯å¢ƒIDæ¥é€‰æ‹©ç›®æ ‡ç‚¹
        target_index = self.env_id % len(self.scene.targets)
        return self.scene.targets[target_index]

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        
        # é‡ç½®åœºæ™¯
        self.scene.reset_scene(drone_ids=[self.drone_id])
        
        # é‡ç½®çŠ¶æ€
        self.current_step = 0
        self.energy = self.max_energy
        self.reward_system.reset_episode()
        self.previous_position = None
        
        # é‡æ–°åˆ†é…ç›®æ ‡ç‚¹ï¼ˆå¢åŠ éšæœºæ€§ï¼‰
        self.target = self._assign_target_by_env_id()
        
        # æ›´æ–°ç»Ÿè®¡
        self.stats['episodes'] += 1
        self.episode_reward = 0  # æ–°å¢ï¼šå›åˆå¼€å§‹æ—¶æ¸…é›¶
        
        obs = self._get_obs()
        info = {
            'env_id': self.env_id,
            'target': self.target,
            'energy': self.energy
        }
        return obs, info

    def step(self, action):
        # é™åˆ¶åŠ¨ä½œå¹…åº¦
        action = np.clip(action, -1, 1)
        
        # è·å–å½“å‰ä½ç½®
        pos = np.array(self.scene.get_drone_position(self.drone_id))
        
        # è®¡ç®—æ–°ä½ç½®
        new_pos = pos + action
        # é™åˆ¶æœ€å¤§é£è¡Œé«˜åº¦
        new_pos[2] = np.clip(new_pos[2], 0.5, 100)
        move_dist = np.linalg.norm(action)
        
        # æ¶ˆè€—èƒ½é‡
        self.energy -= move_dist
        
        # æ£€æŸ¥ä¸éšœç¢ç‰©çš„ç¢°æ’
        collision_occurred = False
        if self.scene.check_collision(new_pos.tolist()):
            self.energy = 0  # ç¢°æ’éšœç¢ç‰©ï¼Œå¤±å»ç”µé‡
            collision_occurred = True
            reward = -500.0  # ç¢°æ’æƒ©ç½š
            reached_target = False  # ä¿®å¤ï¼šä¿è¯åç»­å¼•ç”¨ä¸ä¼šæŠ¥é”™
        else:
            # ç§»åŠ¨æ— äººæœº
            self.scene.move_drone_to(new_pos.tolist(), self.drone_id)
            
            # è®¡ç®—å¥–åŠ±
            reward, reached_target = self.reward_system.get_step_reward(
                self.scene, new_pos.tolist(), [self.target], 
                previous_pos=self.previous_position
            )
            
            # æ›´æ–°ç»Ÿè®¡
            if reached_target:
                self.stats['targets_reached'] += 1
        
        # æ›´æ–°ç»Ÿè®¡
        if collision_occurred:
            self.stats['collisions'] += 1
        
        self.stats['total_reward'] += reward
        self.episode_reward += reward  # æ–°å¢ï¼šç´¯è®¡æœ¬å›åˆå¥–åŠ±
        self.previous_position = pos.tolist()
        
        self.current_step += 1
        
        # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
        terminated = False
        truncated = False
        reached_target_flag = reached_target is not None and reached_target is not False
        if self.energy <= 0:
            terminated = True
        elif self.current_step >= self.max_steps:
            truncated = True
        # æ–°å¢ï¼šåˆ°è¾¾ç›®æ ‡ç‚¹ç«‹å³ç»ˆæ­¢
        if reached_target_flag:
            terminated = True
        # æ›´æ–°å¹³å‡å›åˆé•¿åº¦å’Œæ‰“å°åˆ°è¾¾ç›®æ ‡ç‚¹ä¿¡æ¯
        if terminated or truncated:
            self.stats['avg_episode_length'] = (
                (self.stats['avg_episode_length'] * (self.stats['episodes'] - 1) + self.current_step) 
                / self.stats['episodes']
            )
            if reached_target_flag and terminated:
                print(f"[ç¯å¢ƒ {self.env_id}] åˆ°è¾¾ç›®æ ‡ç‚¹ï¼Œå›åˆç»“æŸï¼Œç”¨æ—¶: {self.current_step} æ­¥ï¼Œæ€»å¥–åŠ±: {self.episode_reward:.2f}ï¼Œç›®æ ‡ç‚¹: {self.target}")
            else:
                print(f"[ç¯å¢ƒ {self.env_id}] å›åˆç»“æŸï¼Œæ€»å¥–åŠ±: {self.episode_reward:.2f}ï¼Œæ­¥æ•°: {self.current_step}ï¼Œç›®æ ‡ç‚¹: {self.target}")  # ä¸­æ–‡è¾“å‡º
        
        info = {
            'env_id': self.env_id,
            'energy': self.energy,
            'steps_taken': self.current_step,
            'collision_occurred': collision_occurred,
            'target_reached': reached_target if 'reached_target' in locals() else False,
            'stats': self.stats.copy()
        }
        
        obs = self._get_obs()
        return obs, reward, terminated, truncated, info

    def _get_obs(self):
        """è·å–è§‚å¯ŸçŠ¶æ€"""
        # è·å–æ— äººæœºä½ç½®
        pos = self.scene.get_drone_position(self.drone_id)
        if pos is None:
            pos = [0, 0, 2.5]
        
        # è·å–é€Ÿåº¦ï¼ˆç®€åŒ–ï¼šä½¿ç”¨ä½ç½®å˜åŒ–ï¼‰
        velocity = [0, 0, 0]
        if self.previous_position is not None:
            velocity = [
                pos[0] - self.previous_position[0],
                pos[1] - self.previous_position[1], 
                pos[2] - self.previous_position[2]
            ]
        
        # å°„çº¿æ£€æµ‹
        ray_distances = self._ray_cast_all_directions(pos)
        
        # æ„å»ºè§‚å¯Ÿå‘é‡
        obs = np.concatenate([
            pos,                    # 3ç»´ï¼šä½ç½®
            self.target,            # 3ç»´ï¼šç›®æ ‡ä½ç½®
            ray_distances,          # 26ç»´ï¼šå°„çº¿è·ç¦»
            velocity               # 3ç»´ï¼šé€Ÿåº¦
        ], dtype=np.float32)
        
        return obs

    def _ray_cast_all_directions(self, drone_pos):
        """åœ¨æ‰€æœ‰26ä¸ªæ–¹å‘è¿›è¡Œå°„çº¿æ£€æµ‹"""
        distances = []
        
        for direction in self.ray_directions:
            # è®¡ç®—å°„çº¿ç»ˆç‚¹
            end_pos = [
                drone_pos[0] + direction[0] * self.ray_length,
                drone_pos[1] + direction[1] * self.ray_length,
                drone_pos[2] + direction[2] * self.ray_length
            ]
            
            # è¿›è¡Œå°„çº¿æ£€æµ‹
            result = p.rayTest(drone_pos, end_pos)[0]
            
            if result[0] == -1:  # æ²¡æœ‰ç¢°æ’
                distance = self.ray_length
            else:
                # è®¡ç®—ç¢°æ’è·ç¦»
                hit_pos = result[3]
                distance = np.linalg.norm(np.array(hit_pos) - np.array(drone_pos))
            
            distances.append(distance)
        
        return np.array(distances, dtype=np.float32)

    def render(self):
        """æ¸²æŸ“ç¯å¢ƒï¼ˆåœ¨æ— å¤´æ¨¡å¼ä¸‹ä¸æ‰§è¡Œï¼‰"""
        pass

def make_single_drone_env(env_id):
    """åˆ›å»ºå•ä¸ªæ— äººæœºç¯å¢ƒçš„å·¥å‚å‡½æ•°"""
    def _init():
        return SingleDronePathPlanningEnv(env_id=env_id)
    return _init

def create_massive_single_drone_envs(num_envs=32, num_cpu=8):
    """åˆ›å»ºå¤§è§„æ¨¡å•æ— äººæœºç¯å¢ƒ"""
    print(f"ğŸš€ åˆ›å»º {num_envs} ä¸ªå•æ— äººæœºç¯å¢ƒ...")
    print(f"ğŸ’» ä½¿ç”¨ {num_cpu} ä¸ªCPUè¿›ç¨‹")
    
    # åˆ›å»ºç¯å¢ƒåˆ—è¡¨
    env_fns = [make_single_drone_env(i) for i in range(num_envs)]
    
    # åˆ›å»ºå‘é‡åŒ–ç¯å¢ƒ - SubprocVecEnvä¼šè‡ªåŠ¨ä½¿ç”¨å¤šè¿›ç¨‹
    env = SubprocVecEnv(env_fns, start_method='fork')
    
    print(f"âœ… æˆåŠŸåˆ›å»º {num_envs} ä¸ªå¹¶è¡Œç¯å¢ƒ")
    print(f"ğŸ“Š è§‚å¯Ÿç©ºé—´ç»´åº¦: {env.observation_space.shape}")
    print(f"ğŸ¯ åŠ¨ä½œç©ºé—´ç»´åº¦: {env.action_space.shape}")
    
    return env

if __name__ == "__main__":
    # æµ‹è¯•å•ä¸ªç¯å¢ƒ
    print("ğŸ§ª æµ‹è¯•å•ä¸ªæ— äººæœºç¯å¢ƒ...")
    env = SingleDronePathPlanningEnv(env_id=0)
    
    # æµ‹è¯•é‡ç½®
    obs, info = env.reset()
    print(f"âœ… ç¯å¢ƒé‡ç½®æˆåŠŸï¼Œè§‚å¯Ÿç»´åº¦: {obs.shape}")
    print(f"ğŸ¯ ç›®æ ‡ä½ç½®: {info['target']}")
    
    # æµ‹è¯•åŠ¨ä½œ
    action = np.random.uniform(-1, 1, 3)
    obs, reward, terminated, truncated, info = env.step(action)
    print(f"âœ… åŠ¨ä½œæ‰§è¡ŒæˆåŠŸï¼Œå¥–åŠ±: {reward:.2f}")
    
    print("ğŸ‰ å•ä¸ªç¯å¢ƒæµ‹è¯•å®Œæˆï¼")
    
    # æµ‹è¯•å¤§è§„æ¨¡ç¯å¢ƒ
    print("\nğŸš€ æµ‹è¯•å¤§è§„æ¨¡å¹¶è¡Œç¯å¢ƒ...")
    try:
        massive_env = create_massive_single_drone_envs(num_envs=8, num_cpu=4)
        
        # æµ‹è¯•æ‰¹é‡é‡ç½®
        obs = massive_env.reset()
        print(f"âœ… æ‰¹é‡é‡ç½®æˆåŠŸï¼Œè§‚å¯Ÿå½¢çŠ¶: {obs.shape}")
        
        # æµ‹è¯•æ‰¹é‡åŠ¨ä½œ
        actions = np.random.uniform(-1, 1, (8, 3))
        # ä¿®å¤ï¼šSubprocVecEnv.step è¿”å›4ä¸ªå€¼ï¼ˆobs, rewards, dones, infosï¼‰
        obs, rewards, dones, infos = massive_env.step(actions)
        print(f"âœ… æ‰¹é‡åŠ¨ä½œæ‰§è¡ŒæˆåŠŸï¼Œå¥–åŠ±å½¢çŠ¶: {rewards.shape}")
        
        print("ğŸ‰ å¤§è§„æ¨¡ç¯å¢ƒæµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ å¤§è§„æ¨¡ç¯å¢ƒæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc() 