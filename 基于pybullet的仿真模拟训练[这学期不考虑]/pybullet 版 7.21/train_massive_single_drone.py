#!/usr/bin/env python3
"""å¤§è§„æ¨¡å•æ— äººæœºè®­ç»ƒè„šæœ¬ - ä½¿ç”¨å¤§é‡å¹¶è¡Œç¯å¢ƒæé«˜GPUåˆ©ç”¨ç‡"""

import argparse
import os
import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from stable_baselines3.common.monitor import Monitor
from massive_single_drone_env import create_massive_single_drone_envs, SingleDronePathPlanningEnv
from utils import auto_load_model, save_model_by_steps

def train_massive_single_drone(
    num_envs=32,
    num_cpu=8,
    timesteps=500000,
    batch_size=256,
    learning_rate=3e-4,
    save_interval=50000,
    eval_interval=25000,
    model_name="massive_single_drone_ppo"
):
    """è®­ç»ƒå¤§è§„æ¨¡å•æ— äººæœºæ¨¡å‹"""
    
    print("ğŸš€ å¼€å§‹å¤§è§„æ¨¡å•æ— äººæœºè®­ç»ƒ...")
    print(f"ğŸ“Š è®­ç»ƒå‚æ•°:")
    print(f"   ç¯å¢ƒæ•°é‡: {num_envs}")
    print(f"   CPUè¿›ç¨‹æ•°: {num_cpu}")
    print(f"   æ€»æ—¶é—´æ­¥: {timesteps}")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   å­¦ä¹ ç‡: {learning_rate}")
    
    # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
    print("\nğŸ”§ åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    train_env = create_massive_single_drone_envs(num_envs=num_envs, num_cpu=num_cpu)
    
    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒï¼ˆä½¿ç”¨è¾ƒå°‘çš„è¿›ç¨‹ï¼‰
    print("ğŸ”§ åˆ›å»ºè¯„ä¼°ç¯å¢ƒ...")
    eval_env = create_massive_single_drone_envs(num_envs=8, num_cpu=4)
    
    # åˆ›å»ºæ¨¡å‹ç›®å½•
    os.makedirs("models", exist_ok=True)
    
    # é…ç½®PPOå‚æ•°
    model_kwargs = {
        "learning_rate": learning_rate,
        "batch_size": batch_size,
        "n_steps": 2048,  # æ¯ä¸ªç¯å¢ƒæ”¶é›†çš„æ­¥æ•°
        "n_epochs": 10,   # æ¯æ¬¡æ›´æ–°çš„è®­ç»ƒè½®æ•°
        "gamma": 0.99,    # æŠ˜æ‰£å› å­
        "gae_lambda": 0.95,
        "clip_range": 0.2,
        "clip_range_vf": None,
        "normalize_advantage": True,
        "ent_coef": 0.01,  # ç†µç³»æ•°ï¼Œé¼“åŠ±æ¢ç´¢
        "vf_coef": 0.5,    # ä»·å€¼å‡½æ•°ç³»æ•°
        "max_grad_norm": 0.5,
        "use_sde": False,
        "sde_sample_freq": -1,
        "target_kl": None,
        "tensorboard_log": None,  # é¿å…tensorboardä¾èµ–é—®é¢˜
        "policy_kwargs": {
            "net_arch": dict(pi=[256, 256], vf=[256, 256]),  # æ›´å¤§çš„ç½‘ç»œ
        },
        "verbose": 1
    }
    
    # åˆ›å»ºPPOæ¨¡å‹ï¼ˆæ–­ç‚¹ç»­è®­ï¼‰
    import torch
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model = auto_load_model(model_name, "models", train_env, device)
    if model is None:
        print("ğŸ¤– åˆ›å»ºPPOæ¨¡å‹...")
        model = PPO(
            "MlpPolicy",
            train_env,
            **model_kwargs
        )
    
    # åˆ›å»ºå›è°ƒå‡½æ•°
    callbacks = []
    
    # è¯„ä¼°å›è°ƒ
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=f"models/{model_name}_best",
        log_path=f"models/{model_name}_eval",
        eval_freq=max(eval_interval // num_envs, 1),
        deterministic=True,
        render=False
    )
    callbacks.append(eval_callback)
    
    # æ£€æŸ¥ç‚¹å›è°ƒ
    checkpoint_callback = CheckpointCallback(
        save_freq=max(save_interval // num_envs, 1),
        save_path=f"models/{model_name}_checkpoints",
        name_prefix=model_name
    )
    callbacks.append(checkpoint_callback)
    
    # å¼€å§‹è®­ç»ƒ
    print(f"\nğŸ¯ å¼€å§‹è®­ç»ƒ {timesteps} æ—¶é—´æ­¥...")
    print("â±ï¸  é¢„è®¡è®­ç»ƒæ—¶é—´: æ ¹æ®ç¯å¢ƒæ•°é‡å’ŒGPUæ€§èƒ½è€Œå®š")
    
    try:
        # åˆ›å»ºè‡ªå®šä¹‰å›è°ƒåˆ—è¡¨ï¼Œé¿å…è¿›åº¦æ¡
        from stable_baselines3.common.callbacks import CallbackList
        callback_list = CallbackList(callbacks)
        
        model.learn(
            total_timesteps=timesteps,
            callback=callback_list,
            progress_bar=False
        )
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        save_model_by_steps(model, "models", model_name)
        print(f"âœ… è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹ä¿å­˜è‡³: models/{model_name}_{model.num_timesteps}.zip")
        
        # æ‰“å°è®­ç»ƒç»Ÿè®¡
        print("\nğŸ“Š è®­ç»ƒç»Ÿè®¡:")
        print(f"   æ€»ç¯å¢ƒæ•°: {num_envs}")
        print(f"   æ€»æ—¶é—´æ­¥: {timesteps}")
        print(f"   å®é™…è®­ç»ƒæ­¥æ•°: {model.num_timesteps}")
        
        return model
        
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        # ä¿å­˜ä¸­æ–­æ—¶çš„æ¨¡å‹
        save_model_by_steps(model, "models", model_name, suffix="interrupted")
        print(f"ğŸ’¾ ä¸­æ–­æ¨¡å‹ä¿å­˜è‡³: models/{model_name}_{model.num_timesteps}_interrupted.zip")
        return model
        
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_trained_model(model_path, num_test_episodes=5):
    """æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print(f"\nğŸ§ª æµ‹è¯•æ¨¡å‹: {model_path}")
    
    try:
        # åŠ è½½æ¨¡å‹
        model = PPO.load(model_path)
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ
        test_env = SingleDronePathPlanningEnv(env_id=0)
        
        total_reward = 0
        success_count = 0
        
        for episode in range(num_test_episodes):
            obs, info = test_env.reset()
            episode_reward = 0
            steps = 0
            
            while True:
                action, _ = model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = test_env.step(action)
                episode_reward += reward
                steps += 1
                
                if terminated or truncated:
                    break
            
            total_reward += episode_reward
            if info.get('target_reached', False):
                success_count += 1
            
            print(f"   å›åˆ {episode + 1}: å¥–åŠ±={episode_reward:.2f}, æ­¥æ•°={steps}, æˆåŠŸ={info.get('target_reached', False)}")
        
        avg_reward = total_reward / num_test_episodes
        success_rate = success_count / num_test_episodes
        
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
        print(f"   å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        print(f"   æˆåŠŸç‡: {success_rate:.2%}")
        
        return avg_reward, success_rate
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        return None, None

def main():
    parser = argparse.ArgumentParser(description="å¤§è§„æ¨¡å•æ— äººæœºè®­ç»ƒ")
    parser.add_argument("--num_envs", type=int, default=32, help="å¹¶è¡Œç¯å¢ƒæ•°é‡")
    parser.add_argument("--num_cpu", type=int, default=8, help="CPUè¿›ç¨‹æ•°")
    parser.add_argument("--timesteps", type=int, default=500000, help="è®­ç»ƒæ—¶é—´æ­¥")
    parser.add_argument("--batch_size", type=int, default=256, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--learning_rate", type=float, default=3e-4, help="å­¦ä¹ ç‡")
    parser.add_argument("--save_interval", type=int, default=50000, help="ä¿å­˜é—´éš”")
    parser.add_argument("--eval_interval", type=int, default=25000, help="è¯„ä¼°é—´éš”")
    parser.add_argument("--model_name", type=str, default="massive_single_drone_ppo", help="æ¨¡å‹åç§°")
    parser.add_argument("--test_only", action="store_true", help="ä»…æµ‹è¯•æ¨¡å‹")
    parser.add_argument("--model_path", type=str, help="è¦æµ‹è¯•çš„æ¨¡å‹è·¯å¾„")
    
    args = parser.parse_args()
    
    if args.test_only:
        if args.model_path:
            test_trained_model(args.model_path)
        else:
            print("âŒ æµ‹è¯•æ¨¡å¼éœ€è¦æŒ‡å®šæ¨¡å‹è·¯å¾„ (--model_path)")
        return
    
    # å¼€å§‹è®­ç»ƒ
    model = train_massive_single_drone(
        num_envs=args.num_envs,
        num_cpu=args.num_cpu,
        timesteps=args.timesteps,
        batch_size=args.batch_size,
        learning_rate=args.learning_rate,
        save_interval=args.save_interval,
        eval_interval=args.eval_interval,
        model_name=args.model_name
    )
    
    if model is not None:
        # æµ‹è¯•æœ€ç»ˆæ¨¡å‹
        final_model_path = f"models/{args.model_name}_{model.num_timesteps}.zip"
        if os.path.exists(final_model_path):
            print(f"\nğŸ§ª æµ‹è¯•æœ€ç»ˆæ¨¡å‹...")
            test_trained_model(final_model_path)

if __name__ == "__main__":
    main() 