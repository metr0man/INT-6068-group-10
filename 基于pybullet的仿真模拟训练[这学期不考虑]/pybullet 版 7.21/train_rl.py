import argparse
import os
import glob
import numpy as np
import matplotlib.pyplot as plt
from stable_baselines3 import PPO
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
from stable_baselines3.common.logger import configure
from drone_rl_env import DronePathPlanningEnv, DroneDynamicsControlEnv
from utils import auto_load_model, save_model_by_steps

def find_latest_checkpoint(save_dir, model_name):
    """æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
    checkpoint_pattern = os.path.join(save_dir, f"{model_name}_*_steps.zip")
    checkpoints = glob.glob(checkpoint_pattern)
    
    if not checkpoints:
        return None
    
    # æŒ‰æ–‡ä»¶åä¸­çš„æ­¥æ•°æ’åºï¼Œè¿”å›æœ€æ–°çš„
    checkpoints.sort(key=lambda x: int(x.split('_')[-2]))
    return checkpoints[-1]

def find_best_model(save_dir, model_name):
    """æŸ¥æ‰¾æœ€ä½³æ¨¡å‹æ–‡ä»¶"""
    best_model_path = os.path.join(save_dir, f"{model_name}_best_model.zip")
    if os.path.exists(best_model_path):
        return best_model_path
    return None

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--env', type=str, default='path', choices=['path', 'dynamics'], help='è®­ç»ƒç¯å¢ƒç±»å‹')
    parser.add_argument('--timesteps', type=int, default=100000, help='è®­ç»ƒæ­¥æ•°')
    parser.add_argument('--save_dir', type=str, default='./rl_models', help='æ¨¡å‹ä¿å­˜ç›®å½•')
    parser.add_argument('--continue_training', action='store_true', help='ç»§ç»­ä¹‹å‰çš„è®­ç»ƒ')
    parser.add_argument('--force_new', action='store_true', help='å¼ºåˆ¶é‡æ–°å¼€å§‹è®­ç»ƒ')
    args = parser.parse_args()

    os.makedirs(args.save_dir, exist_ok=True)
    
    # é…ç½®æ—¥å¿—
    configure(args.save_dir, ["stdout", "csv"])

    if args.env == 'path':
        env = DronePathPlanningEnv()
        model_name = 'ppo_path_planning'
    else:
        env = DroneDynamicsControlEnv()
        model_name = 'ppo_dynamics_control'

    env = Monitor(env)
    
    # åˆ›å»ºå›è°ƒå‡½æ•°
    eval_callback = EvalCallback(env, best_model_save_path=args.save_dir,
                                 log_path=args.save_dir, eval_freq=5000,
                                 deterministic=True, render=False)

    checkpoint_callback = CheckpointCallback(save_freq=10000, save_path=args.save_dir,
                                           name_prefix=model_name)

    print(f"å¼€å§‹è®­ç»ƒ {model_name} æ¨¡å‹...")
    print(f"è®­ç»ƒæ­¥æ•°: {args.timesteps}")
    print(f"ä¿å­˜ç›®å½•: {args.save_dir}")
    
    # æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨
    import torch
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name()}")
    
    # ç»Ÿä¸€æ¨¡å‹åŠ è½½é€»è¾‘
    model = None
    if not args.force_new:
        model = auto_load_model(model_name, args.save_dir, env, device) if args.continue_training else None
    if model is None:
        print("ğŸ†• åˆ›å»ºæ–°æ¨¡å‹...")
        model = PPO('MlpPolicy', env, verbose=1, device=device)
    
    # å¼€å§‹è®­ç»ƒ
    model.learn(total_timesteps=args.timesteps, 
                callback=[eval_callback, checkpoint_callback])
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    save_model_by_steps(model, args.save_dir, model_name)
    
    # æ˜¾ç¤ºè®­ç»ƒå®Œæˆä¿¡æ¯
    print(f"\nğŸ‰ è®­ç»ƒå®Œæˆ!")
    print(f"æœ€ç»ˆæ¨¡å‹: {os.path.join(args.save_dir, model_name + '_' + str(model.num_timesteps) + '.zip')}")
    print(f"æœ€ä½³æ¨¡å‹: {os.path.join(args.save_dir, model_name + '_best_model.zip')}")
    print(f"æ£€æŸ¥ç‚¹æ–‡ä»¶: {args.save_dir}/") 