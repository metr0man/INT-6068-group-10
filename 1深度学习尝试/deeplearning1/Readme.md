## æœ¬æ–‡æ¡£å‡è®¾è¯»è€…å®Œå…¨æ²¡æœ‰RLèƒŒæ™¯ï¼Œå°½é‡ç”¨é€šä¿—æ˜“æ‡‚çš„è¯­è¨€è§£é‡Šå¤æ‚æ¦‚å¿µï¼Œè®©æ–°æ‰‹èƒ½å¤Ÿå¿«é€Ÿä¸Šæ‰‹å¹¶ç†è§£å¼ºåŒ–å­¦ä¹ çš„æ ¸å¿ƒæ€æƒ³ã€‚

### ğŸ“ é¡¹ç›®ç»“æ„
```
deeplearning1/ â† ä½ ç°åœ¨åœ¨è¿™é‡Œï¼ 
â”œâ”€â”€ environment.py # ğŸŒ ç¯å¢ƒæ–‡ä»¶ - å®šä¹‰æ— äººæœºä¸–ç•Œ 
â”œâ”€â”€ td3.py # ğŸ§  TD3ç®—æ³• - æ™ºèƒ½ä½“çš„å¤§è„‘ 
â”œâ”€â”€ model.py # ğŸ—ï¸ ç¥ç»ç½‘ç»œ - å¤§è„‘çš„å…·ä½“ç»“æ„ 
â”œâ”€â”€ train.py # ğŸƒ è®­ç»ƒè„šæœ¬ - å¼€å§‹è®­ç»ƒï¼ 
â”œâ”€â”€ analysis.py # ğŸ“Š åˆ†æå·¥å…· - çœ‹æ‡‚è®­ç»ƒç»“æœ 
â”œâ”€â”€ training_log.csv # ğŸ“ˆ è®­ç»ƒæ•°æ® - è®°å½•å­¦ä¹ è¿‡ç¨‹ 
â”œâ”€â”€ reward_curve.png # ğŸ“Š å¥–åŠ±æ›²çº¿ - å¯è§†åŒ–è¿›æ­¥ 
â””â”€â”€ td3_actor.pth # ğŸ’¾ è®­ç»ƒå¥½çš„æ¨¡å‹ - ä½ çš„æˆæœ
```

### æœ¬æ¬¡æ·±åº¦å­¦ä¹ ç®—æ³•é‡‡ç”¨ç±»ä¼¼DDPGçš„[TD3ç®—æ³•](https://www.bilibili.com/video/BV1kP411X78D)å®ç°ã€‚
> æ— äººæœºæ˜¯**è¿ç»­åŠ¨ä½œæ§åˆ¶**ï¼ˆx/y/z ä¸‰ç»´é€Ÿåº¦ï¼Œä¸æ˜¯ä¸Šä¸‹å·¦å³ 4 ä¸ªé”®ï¼‰ï¼Œ
	TD3 å°±æ˜¯ç›®å‰**æœ€é€‚åˆè¿ç»­æ§åˆ¶**çš„ RL ç®—æ³•ä¹‹ä¸€ï¼Œæ¯” DQN æ›´é€‚åˆé£æ— äººæœºã€‚
- **ç®—æ³•å¯¹æ¯”ä¸é€‚ç”¨åœºæ™¯**

|   ç®—æ³•    |     æ ¸å¿ƒä¼˜åŠ¿      |      ä¸»è¦ç¼ºç‚¹      |      é€‚ç”¨åœºæ™¯       |
| :-----: | :-----------: | :------------: | :-------------: |
|  DDPG   |   é€‚ç”¨äºè¿ç»­åŠ¨ä½œç©ºé—´   |   æ˜“è¿‡ä¼°è®¡ï¼Œç¨³å®šæ€§å·®    |     ç®€å•æœºå™¨äººæ§åˆ¶     |
| **PPO** | **æ€§èƒ½ä¼˜å¼‚ï¼Œå®ç°ç®€å•** | **ç†è®ºåŸºç¡€ä¸å¦‚TRPO** | **å‡ ä¹æ‰€æœ‰åœºæ™¯ï¼Œæ¨èé¦–é€‰** |
|  TRPO   |   ä¿è¯ç­–ç•¥å•è°ƒæå‡    |   å®ç°å¤æ‚ï¼Œè®¡ç®—é‡å¤§    |  éœ€è¦ä¸¥æ ¼ä¿è¯ç¨³å®šæ€§çš„åœºæ™¯   |
|   SAC   |  æ¢ç´¢èƒ½åŠ›å¼ºï¼Œæ ·æœ¬æ•ˆç‡é«˜  |      è°ƒå‚å¤æ‚      |    éœ€è¦å……åˆ†æ¢ç´¢çš„ç¯å¢ƒ    |
|   **TD3**   |   **æ¯”DDPGæ›´ç¨³å®š**    |     **æ¯”PPOå¤æ‚**     |  **è¿ç»­åŠ¨ä½œç©ºé—´ï¼Œéœ€è¦é«˜ç¨³å®šæ€§**  |
| MADDPG  |   é€‚ç”¨äºå¤šæ™ºèƒ½ä½“ç³»ç»Ÿ   |   æ‰©å±•æ€§å·®ï¼Œéœ€å¤§é‡è®­ç»ƒ   |   å¤šæœºå™¨äººåä½œï¼Œæ¸¸æˆAI   |

### [TD3ç®—æ³•](https://www.bilibili.com/video/BV1kP411X78D)åŸç†é€Ÿé€šğŸ¤“
 **TD3å¥½æ¯”åšæ™ºèƒ½ä½“ / ç®—æ³• / æ§åˆ¶å™¨**ï¼Œä¸»è¦è´Ÿè´£ï¼šçœ‹çŠ¶æ€ â†’ åšå†³ç­– â†’ å­¦æ€ä¹ˆé£å¾—æ›´å¥½ â†’ è¾“å‡ºæ§åˆ¶æŒ‡ä»¤ã€‚

TD3é‡Œé¢æœ‰ä¸¤å¥—ç½‘ç»œï¼š
- **Actorï¼ˆæ¼”å‘˜ / ç­–ç•¥ç½‘ç»œï¼‰**
- ![[Pasted image 20260228164137.png]]
        çœ‹çŠ¶æ€ â†’ è¾“å‡ºåŠ¨ä½œ
        â†’ å¯¹åº”ï¼šæ— äººæœºçš„ **â€œæ‰‹å’Œè„šâ€**
        ç®€å•åœ°è®²è¢«å¼ºåŒ–å­¦ä¹ çš„ğŸ±
- **Criticï¼ˆè¯„è®ºå®¶ / ä»·å€¼ç½‘ç»œï¼‰**
- ![[9b3cf8c7-193c-4a6c-b798-41264cc369ca.png]]
        è¯„ä»·è¿™ä¸ªåŠ¨ä½œå¥½ä¸å¥½
        â†’ å¯¹åº”ï¼šæ— äººæœºçš„ **â€œè€å¸ˆ / æ•™ç»ƒâ€**
        ç®€å•åœ°è®²å°±æ˜¯ç»™æ¯æ¬¡å¼ºåŒ–å­¦ä¹ æ‰“åˆ†çš„ï¼Œç†è§£ä¸º **â€œçœŸæ£’ï¼â€** å°±å¥½

è¿˜æœ‰å‡ ä¸ªå…³é”®åŠŸèƒ½ï¼š

- `select_action()`ï¼š**æ‹¿ä¸»æ„** â†’ å‘Šè¯‰æ— äººæœºç°åœ¨æ€ä¹ˆé£
- `update()`ï¼š**å­¦ä¹ æ›´æ–°** â†’ ä»ç»éªŒé‡Œæ”¹é”™ã€å˜å¼º

## ğŸš€ 5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹


### ç¬¬1æ­¥ï¼šè¿è¡Œä½ çš„ç¬¬ä¸€ä¸ªè®­ç»ƒ
## [ç‚¹å‡»é“¾æ¥è·³è½¬Colabï¼Œé€æ­¥è¿è¡Œå³å¯é¢„è§ˆç›¸åŒæ•ˆæœï¼Œçœå»ç¯å¢ƒéƒ¨ç½²ï¼ˆä½†æ˜¯è·‘å®Œè¦å¾ˆä¹…ï¼‰](https://colab.research.google.com/drive/1do7k5KFhJ-CpMvLQHdmNUIqYWpbaNtfp#scrollTo=ZqEK_3Anj6BC)

```bash
python train.py
```

ä½ ä¼šçœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„è¾“å‡ºï¼š
````
Episode 0 | Reward: -15.23 Episode 1 | Reward: -12.45 Episode 2 | Reward: -8.67 ... Episode 999 | Reward: -0.89
````

**ğŸ’¡ å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ**
- å¥–åŠ±ï¼ˆRewardï¼‰ä»è´Ÿå€¼é€æ¸æ¥è¿‘0ï¼Œè¯´æ˜æ— äººæœºå­¦å¾—è¶Šæ¥è¶Šå¥½ï¼
- è´Ÿå¥–åŠ±è¡¨ç¤ºæ— äººæœºè·ç¦»ç›®æ ‡è¿˜æœ‰è·ç¦»
- è¶Šæ¥è¿‘0ï¼Œè¯´æ˜æ— äººæœºè¶Šæ¥è¿‘ç›®æ ‡

### ç¬¬2æ­¥ï¼šæŸ¥çœ‹è®­ç»ƒç»“æœ
è®­ç»ƒå®Œæˆåï¼Œä½ ä¼šå¾—åˆ°ï¼š
```
- `training_log.csv` - è¯¦ç»†è®­ç»ƒæ•°æ®
- `reward_curve.png` - å¥–åŠ±å˜åŒ–æ›²çº¿
- `td3_actor.pth` - è®­ç»ƒå¥½çš„æ™ºèƒ½ä½“æ¨¡å‹
```
## ğŸ” æ¯ä¸ªæ–‡ä»¶è¯¦è§£

### ğŸŒ environment.py - æ— äººæœºç¯å¢ƒ
```python
# æ ¸å¿ƒæ¦‚å¿µï¼šç¯å¢ƒ = æ— äººæœº + ç›®æ ‡ + ç‰©ç†è§„åˆ™
class DroneEnv:
    def reset(self):        # é‡ç½®ç¯å¢ƒåˆ°åˆå§‹çŠ¶æ€
    def step(self, action): # æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å›æ–°çŠ¶æ€å’Œå¥–åŠ±
    def _get_state(self):   # è·å–å½“å‰çŠ¶æ€ï¼ˆä½ç½®+ç›®æ ‡æ–¹å‘ï¼‰
```

**å…³é”®å‚æ•°ï¼ˆä½ å¯ä»¥ä¿®æ”¹çš„ï¼ï¼‰ï¼š**
```python
self.action_dim = 3      # åŠ¨ä½œç»´åº¦ï¼šx,y,zä¸‰ä¸ªæ–¹å‘ç§»åŠ¨
self.state_dim = 6       # çŠ¶æ€ç»´åº¦ï¼šä½ç½®(3) + ç›®æ ‡æ–¹å‘(3)
self.max_step = 200      # æ¯å›åˆæœ€å¤§æ­¥æ•°
```

### ğŸ§  td3.py - æ™ºèƒ½ä½“å¤§è„‘
```python
# æ ¸å¿ƒæ¦‚å¿µï¼šTD3 = Twin Delayed Deep Deterministic Policy Gradient
class TD3:
    def select_action(self, state):  # æ ¹æ®çŠ¶æ€é€‰æ‹©åŠ¨ä½œ
    def update(self, replay_buffer): # ä»ç»éªŒä¸­å­¦ä¹ 
```

**å…³é”®è¶…å‚æ•°ï¼ˆè°ƒå‚é‡ç‚¹ï¼ï¼‰ï¼š**
```python
lr = 3e-4              # å­¦ä¹ ç‡ï¼šå­¦ä¹ é€Ÿåº¦
discount = 0.99        # æŠ˜æ‰£å› å­ï¼šæœªæ¥å¥–åŠ±çš„é‡è¦æ€§
policy_noise = 0.2     # æ¢ç´¢å™ªå£°ï¼šå°è¯•æ–°äº‹ç‰©çš„ç¨‹åº¦
policy_freq = 2        # æ›´æ–°é¢‘ç‡ï¼šå¤šä¹…æ›´æ–°ä¸€æ¬¡ç­–ç•¥
```

### ğŸƒ train.py - è®­ç»ƒè¿‡ç¨‹
```python
# è®­ç»ƒå¾ªç¯ï¼šä¸ç¯å¢ƒäº¤äº’ â†’ æ”¶é›†ç»éªŒ â†’ æ›´æ–°ç­–ç•¥
for episode in range(max_episodes):
    state = env.reset()
    for t in range(200):
        action = agent.select_action(state)
        next_state, reward, done, _ = env.step(action)
        replay_buffer.add(state, action, reward, next_state, done)
        if len(replay_buffer) > 1000:
            agent.update(replay_buffer)
```

### ğŸ“Š analysis.py - ç»“æœåˆ†æ
```python
# åˆ†æè®­ç»ƒç»“æœï¼Œç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
def plot_reward_curve():     # ç»˜åˆ¶å¥–åŠ±æ›²çº¿
def analyze_training_log():  # åˆ†æè®­ç»ƒæ—¥å¿—
def test_trained_model():    # æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹
```

## ğŸ® ç†è§£æ ¸å¿ƒæ¦‚å¿µ

### ğŸ”„ å¼ºåŒ–å­¦ä¹ å¾ªç¯
````