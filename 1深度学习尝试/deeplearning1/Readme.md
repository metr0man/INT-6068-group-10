## 深度学习绕不开的算法有二，PPO和DDPG。

- **算法对比与适用场景**

|   算法    |     核心优势      |      主要缺点      |      适用场景       |
| :-----: | :-----------: | :------------: | :-------------: |
|  DDPG   |   适用于连续动作空间   |   易过估计，稳定性差    |     简单机器人控制     |
| **PPO** | **性能优异，实现简单** | **理论基础不如TRPO** | **几乎所有场景，推荐首选** |
|  TRPO   |   保证策略单调提升    |   实现复杂，计算量大    |  需要严格保证稳定性的场景   |
|   SAC   |  探索能力强，样本效率高  |      调参复杂      |    需要充分探索的环境    |
|   **TD3**   |   **比DDPG更稳定**    |     **比PPO复杂**     |  **连续动作空间，需要高稳定性**  |
| MADDPG  |   适用于多智能体系统   |   扩展性差，需大量训练   |   多机器人协作，游戏AI   |
    
### 本次深度学习算法采用类似DDPG的[TD3算法](https://www.bilibili.com/video/BV1kP411X78D)实现。

这一段 **TD3 代码**：

是**智能体 / 算法 / 控制器**，负责：看状态 → 做决策 → 学怎么飞得更好 → 输出控制指令。
# 这段代码在本项目里的**4 个核心作用**

## 1. 实现了 **TD3 强化学习算法**（专门控无人机的算法）

你的无人机是**连续动作控制**（x/y/z 三维速度，不是上下左右 4 个键），

TD3 就是目前**最适合连续控制**的 RL 算法之一，比 DQN 更适合飞无人机。
TD3里面有两套网络：
- **Actor（演员 / 策略网络）**
- ![[Pasted image 20260228164137.png]]
        看状态 → 输出动作
        → 对应：无人机的**“手和脚”**
        简单地讲被强化学习的
- **Critic（评论家 / 价值网络）**
- ![[9b3cf8c7-193c-4a6c-b798-41264cc369ca.png]]
        评价这个动作好不好
        → 对应：无人机的**“老师 / 教练”**
        简单地讲就是给每次强化学习打分的

还有几个关键功能：

- `select_action()`：**拿主意** → 告诉无人机现在怎么飞
- `update()`：**学习更新** → 从经验里改错、变强


## 2. 给无人机提供**“决策大脑”**

- 输入：无人机看到的环境（位置、目标、障碍物、其他无人机）
- 输出：无人机下一步该怎么飞（3 维动作）

## 3. 负责**学习和进步**

它会根据环境给的**奖励**（到达目标加分、撞墙扣分、耗电扣分），

自动调整神经网络，越飞越好：

- 学会避障
- 学会抄近路
- 学会省电
- 学会不和其他无人机相撞

## 4. 提供**经验记忆库**，让训练更稳定

代码里的 `ReplayBuffer` 是**经验回放池**，把无人机飞过的所有经历存起来，

打乱再学习，防止学崩，这是深度强化学习**必用组件**。
